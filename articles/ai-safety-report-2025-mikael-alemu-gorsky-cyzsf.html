<html>
<head>
  <title>AI Safety Report 2025</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img class="series-logo" src="https://media.licdn.com/dms/image/v2/D4D12AQEfmvWDvno3mg/series-logo_image-shrink_300_300/series-logo_image-shrink_300_300/0/1726763056922?e=1754524800&v=beta&t=yDjwvI9T-dZ4fE5WvZZELb-Qjf3wgc7WRrFECKBQX9Q" />
    <h3>
      <span class="series-title">The AI Pravda</span>
      </br>
      <span class="series-description">Generative AI will change the world as we know it. Let's get prepared. </span>
    </h3>
    <img src="https://media.licdn.com/mediaD4D12AQFCMNi1cRH8Wg" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/ai-safety-report-2025-mikael-alemu-gorsky-cyzsf">AI Safety Report 2025</a></h1>
    <p class="created">Created on 2025-02-05 20:33</p>
  <p class="published">Published on 2025-02-06 05:45</p>
  <div><p>The <strong>International AI Safety Report 2025</strong> was published in <strong>January 2025</strong> as the first full edition following an interim version released in <strong>May 2024</strong>. It was authored by <strong>96 international AI experts</strong>, including contributors from <strong>30 governments</strong>, as well as representatives from the <strong>United Nations (UN), the European Union (EU), and the Organisation for Economic Co-operation and Development (OECD)</strong>. The report was chaired by <strong>Prof. Yoshua Bengio</strong> and coordinated by the <strong>AI Safety Institute</strong>. </p><p>Commissioned as part of the international AI safety initiative launched at the <strong>Bletchley Park AI Safety Summit (2023)</strong> and developed through collaboration with experts from leading research institutions and industry, the report was presented ahead of the <strong>AI Action Summit in Paris (February 2025)</strong>. It serves as a scientific reference for policymakers, synthesizing current research on <strong>AI capabilities, risks, and risk management strategies</strong>. </p><h2>Overview of the Report</h2><p> </p><figure><img data-media-urn="urn:li:digitalmediaAsset:D4D12AQGt_RHOO0shRg" src="https://media.licdn.com/dms/image/v2/D4D12AQGt_RHOO0shRg/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1738787756296?e=1754524800&amp;v=beta&amp;t=ndHUvMqrvJzJQ2ieWlGOqOw64QL3PGUNz648uEBle8o"><figcaption></figcaption></figure><p>The report is built around three main ideas: <strong>what AI can do, what risks it brings, and how to manage those risks</strong>. AI risk management is the final step in this logic, following the explanation of AI’s capabilities and dangers.</p><p>The report starts by describing how <strong>general-purpose AI (GPAI) is developed</strong>. It explains that AI models are trained on vast amounts of data using deep learning techniques, making them capable of generating text, images, and code, as well as answering complex questions. It also discusses <strong>how AI capabilities have evolved rapidly</strong>—from struggling to write a simple paragraph to now outperforming experts in programming and scientific reasoning.</p><p>After setting the stage with capabilities, the report moves into <strong>risks</strong>. It breaks them into three categories: <strong>malicious use, malfunctions, and systemic risks</strong>. Malicious use includes <strong>AI-generated scams, deepfake disinformation, cyberattacks, and even guidance on biological weapons</strong>. Malfunctions cover <strong>bias, misinformation, and AI’s tendency to behave in unpredictable ways</strong>. Systemic risks focus on <strong>the economic and environmental impact of AI, market concentration in a few companies, and the growing gap between AI-rich and AI-poor nations</strong>.</p><p>Only after these risks are laid out does the report turn to <strong>AI risk management</strong>. This part of the report is about <strong>how to prevent, monitor, and mitigate harm</strong> while still allowing AI to be useful. The report acknowledges that <strong>no single approach will be enough</strong>, so it outlines different methods that can be used together. These include <strong>better AI testing before release, continuous monitoring after deployment, improved transparency from AI companies, and global cooperation on regulation</strong>.</p><p>A key theme in the risk management section is that <strong>AI advances faster than our ability to regulate it</strong>. This creates what the report calls the <strong>"evidence dilemma"</strong>—should governments act early, before they have clear proof of harm, or wait for stronger evidence and risk being too late? Some risks, like AI-powered cyberattacks, are already happening and need urgent action. Others, like AI becoming autonomous and uncontrollable, are more speculative but could be catastrophic if they materialize.</p><p>The report does not prescribe exact policies but emphasizes that <strong>governments, companies, and researchers must work together</strong> to find solutions. It also highlights <strong>the growing influence of AI companies</strong> in shaping policy, as they often control key information about AI risks. Some governments push for <strong>strict AI safety testing before deployment</strong>, while others prefer <strong>industry-led self-regulation</strong>. The report presents this debate without taking sides but stresses that <strong>AI will not regulate itself</strong>—human decisions will determine whether it remains beneficial or turns into a global problem. </p><p>In short, <strong>AI risk management is the bridge between identifying dangers and finding ways to live with AI safely</strong>. The report builds up the case for why risk management is necessary, explaining AI’s rapid growth and emerging risks. It then presents current efforts to make AI safer and discusses the challenges that remain. The report leaves readers with a clear message: <strong>AI is here to stay, and managing its risks is not optional—it’s essential for shaping a future where AI works for everyone, not against them</strong>.</p><h2>AI Risk Management </h2><figure><img data-media-urn="urn:li:digitalmediaAsset:D4D12AQGeUFTjIkb5Rw" src="https://media.licdn.com/dms/image/v2/D4D12AQGeUFTjIkb5Rw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1738787890029?e=1754524800&amp;v=beta&amp;t=2I7fp6GSw_xi7eHEJbfTmAHXbGkXcvzAOF0NWDI8LTc"><figcaption></figcaption></figure><p>AI risk management is still in its early stages. The systems we have today are powerful but unpredictable. Even the best safeguards can’t guarantee that AI will behave as expected. The challenge is that AI is evolving quickly, making it hard to create long-term safety measures that actually work. The risks are serious—AI can be used for cyberattacks, spread misinformation, and even help design dangerous biological threats. At the same time, AI is unreliable. It can generate biased answers, violate privacy, or simply make things up with great confidence.</p><p>One of the biggest problems is that AI developers don’t fully understand how their models work. Even when AI gives correct answers, it’s not always clear how it arrived at them. This makes it difficult to predict when an AI system will fail. Testing AI for safety before release is common, but it isn’t perfect. Most tests rely on pre-set scenarios, which don’t always reflect real-world use. AI can also change after deployment. Users can fine-tune models, find loopholes, or exploit weaknesses in ways developers didn’t expect.</p><p>Regulation is difficult because AI companies are moving fast. There’s a race to build the most powerful models, and companies don’t always share safety research with the public. Governments, meanwhile, struggle to keep up. AI research is mostly in the hands of private companies, and policymakers often don’t have access to detailed information. Some governments are trying to introduce transparency rules, requiring AI companies to disclose risks before releasing new models. But companies push back, arguing that too much transparency could harm competition.</p><p>Some AI risks are already clear. Scammers use AI to create deepfake videos and fake voices. Cybercriminals use it to automate attacks. In some experiments, AI has even helped generate instructions for making chemical weapons. Other risks are harder to measure. AI could disrupt the job market, concentrate power in a few companies, or increase global inequality. Many researchers believe AI is progressing so fast that we won’t have enough time to react when serious risks emerge. </p><p>There are efforts to manage these risks. Some companies are working on better training techniques to make AI more trustworthy. Others are developing monitoring systems that track AI behavior in real time. But safety measures have limits. AI-generated content can still bypass detection, and users often find ways to trick safeguards. Governments are discussing risk thresholds—clear warning signs that would trigger restrictions or stricter rules. But without reliable ways to predict risks, it’s hard to know when to act.</p><p> One of the biggest debates is whether we should regulate AI before problems appear or wait for stronger evidence. Acting too early could slow progress. Waiting too long could mean losing control over risks that become impossible to manage. Some governments are taking a cautious approach, focusing on the most obvious dangers, like AI-powered fraud and cybercrime. Others argue that AI could become too powerful too quickly, requiring more aggressive regulation.</p><p>AI risk management will need to evolve. AI agents—systems that plan and act on their own—are becoming more advanced. These systems could complete long-term tasks, interact with other AI, or even modify their own behavior. The risks here are unknown, but experts agree that more research is needed. Some worry that AI could eventually operate beyond human control, though opinions differ on how likely this is.</p><p>The future of AI depends on the choices made today. Governments, researchers, and companies need to find ways to manage risks without slowing innovation too much. AI safety measures need to be flexible, adapting as technology changes. The challenge is keeping up. AI is improving faster than our ability to regulate it, and waiting for a perfect solution might mean running out of time. </p><h2>Responsible AI</h2><figure><img data-media-urn="urn:li:digitalmediaAsset:D4D12AQH3Fp3PYJtsOQ" src="https://media.licdn.com/dms/image/v2/D4D12AQH3Fp3PYJtsOQ/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1738787955773?e=1754524800&amp;v=beta&amp;t=ultKQ-TvlcRetFKov9HyxTMYr1MLkyuo2JSIoDpKz9U"><figcaption></figcaption></figure><p>The <strong>International AI Safety Report 2025</strong> references the concept of <strong>Responsible AI</strong> in several contexts, particularly in discussions on <strong>AI risk management, governance, and deployment strategies</strong>.</p><p>The report acknowledges that <strong>responsible AI development requires clear guidelines and proactive policies</strong>. It mentions efforts by AI companies and researchers to implement <strong>responsible scaling policies</strong>, responsible release strategies, and <strong>risk management frameworks</strong>. These include <strong>acceptable use policies, AI model documentation, and external oversight mechanisms</strong>.</p><p>A key discussion in the report is the <strong>role of industry-led initiatives</strong> in promoting responsible AI. Companies like <strong>Anthropic, Microsoft, and DeepMind</strong> have introduced <strong>responsible AI licensing policies</strong> to prevent AI misuse. These policies set conditions on <strong>how AI models can be used, monitored, and improved</strong> over time.</p><p>The report also connects <strong>responsible AI to biosecurity and safety-critical fields</strong>. It discusses responsible AI practices in <strong>biodesign, cybersecurity, and autonomous systems</strong>. For example, it highlights the need for <strong>responsible AI in medical and scientific research</strong>, ensuring that AI-generated discoveries <strong>follow ethical guidelines</strong>.</p><p>One of the major challenges in implementing responsible AI is <strong>lack of enforcement</strong>. While many companies promote responsible AI in their public statements, <strong>there is little independent oversight</strong> to ensure compliance. The report suggests that <strong>governments should play a more active role</strong> by requiring AI developers to <strong>report safety concerns, disclose risk assessments, and establish clear accountability structures</strong>.</p><p> Overall, the report sees <strong>Responsible AI as an essential but underdeveloped part of AI governance</strong>. While companies are making progress in defining principles and policies, <strong>regulatory frameworks and enforcement mechanisms are still weak</strong>. The report warns that <strong>without stronger accountability, responsible AI efforts may remain voluntary and ineffective</strong>, leaving critical risks unaddressed.</p></div>
</body>
</html>