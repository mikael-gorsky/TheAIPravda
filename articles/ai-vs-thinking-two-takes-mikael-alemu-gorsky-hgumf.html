<html>
<head>
  <title>AI vs. Thinking: two takes</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img class="series-logo" src="https://media.licdn.com/dms/image/v2/D4D12AQEfmvWDvno3mg/series-logo_image-shrink_300_300/series-logo_image-shrink_300_300/0/1726763056922?e=1754524800&v=beta&t=yDjwvI9T-dZ4fE5WvZZELb-Qjf3wgc7WRrFECKBQX9Q" />
    <h3>
      <span class="series-title">The AI Pravda</span>
      </br>
      <span class="series-description">Generative AI will change the world as we know it. Let's get prepared. </span>
    </h3>
    <img src="https://media.licdn.com/mediaD4D12AQHFa5vf4qrz2A" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/ai-vs-thinking-two-takes-mikael-alemu-gorsky-hgumf">AI vs. Thinking: two takes</a></h1>
    <p class="created">Created on 2025-03-16 21:06</p>
  <p class="published">Published on 2025-03-17 05:45</p>
  <div><h2>What’s This Study About?</h2><p>This research, called "The Impact of Generative AI on Critical Thinking," checks out how AI tools like ChatGPT affect how knowledge workers think. It was done by Hao-Ping (Hank) Lee from Carnegie Mellon University and a Microsoft Research team, and it’ll be shared at a big tech conference in Japan in 2025. Knowledge workers are folks whose jobs are all about handling info, like tech people, designers, or office workers.</p><p>The study asks two big questions: when and how do these workers use critical thinking with AI, and does AI make thinking easier or harder? They surveyed 319 workers who use AI tools at least once a week. These people gave 936 real-life examples of using AI at work, like writing reports, finding facts, or getting advice.</p><p>To figure out critical thinking, they used Bloom’s taxonomy, which breaks thinking into six steps: remembering stuff, understanding it, using it, breaking it down, mixing it up, and judging it. They crunched numbers and read workers’ own words to get the full picture. It’s about seeing how AI shakes up real jobs, not fake tests.</p><h2>When and How Workers Think Critically</h2><p>Workers use critical thinking with AI mostly to make sure their work’s solid. The study says they do it more if they trust their own skills and less if they lean hard on AI. Feeling good about yourself or liking to think things over makes you dig deeper.</p><p>So, how do they do it? They set clear goals for AI, fine-tune their questions, and check AI’s answers against other sources or what they already know, especially when it’s got to be spot-on. They see critical thinking as planning what to ask, double-checking what comes back, and tweaking it—like making a chatbot’s draft sound more like them.</p><p>They’re pushed to think hard because they want better work, fewer mistakes, or to level up their skills. But they hit walls like not realizing they need to think more, being too rushed, or not knowing how to fix AI’s goofs. It’s a tug-of-war between trying hard and getting stuck.</p><h2>Does AI Make Thinking Easier or Harder?</h2><p>AI often makes critical thinking feel lighter, especially for workers who trust it a bunch. They say stuff like digging up info or writing first drafts takes way less brainpower with AI tools. It’s like having a helper do the boring parts.</p><p>But workers who believe in themselves more than AI say it’s tougher—they spend extra effort judging or using what AI gives them. They don’t just take it and run; they poke at it to make sure it’s right. It all depends on who you trust: AI or you.</p><p>The study spotted a shift too: AI changes what you work on. Instead of hunting facts, you’re checking them; instead of solving stuff, you’re fitting AI’s answers into your job. They call it “task stewardship”—you’re not doing it all, you’re watching over AI like a boss.</p><h2>What’s Good and Bad About AI Here?</h2><p>AI’s great at saving time—it makes tasks like writing or researching zip by faster. Workers love how it knocks out the dull stuff quick, letting them get more done. The study says this speed boost is a huge plus for busy days.</p><p>But there’s a downside: AI can make workers slack off on thinking, especially on easy jobs where they just nod at what AI says. If you keep doing that, you might get rusty at figuring things out solo. The researchers call it “overreliance”—leaning on AI so much you lose your edge.</p><p>It’s a double-edged sword: AI helps you fly through work but might leave your brain a little lazy. Think of pilots who stopped flying planes by hand because autopilot was too good—that’s the worry here. You’ve got to stay sharp, even with AI pitching in.</p><h2>How Can We Fix This?</h2><p>The researchers have ideas to keep AI from dulling our minds. They say AI tools could nudge us to think—like popping up a note saying, “I’m only 70% sure this is right,” so we double-check it. It’s a little push to keep us awake and questioning.</p><p>Another trick is making AI show us how it got its answers, like a teacher explaining homework. If ChatGPT says, “Here’s why I wrote this,” we can spot mistakes or make it better ourselves. That way, we’re not just swallowing what it spits out—we’re learning from it.</p><p>They also think AI could cheer us on to grow, not just finish fast. Imagine an AI saying, “Hey, use me to get better at this skill,” instead of “Here’s your answer, bye.” It could turn AI into a teammate that helps us shine, not a shortcut that skips the hard stuff.</p><p>What if AI asked us questions back? Like, “Did you check these facts?” or “How would you tweak this?”—it’d make us stop and think instead of cruising on autopilot. For example, if you’re writing a report, AI could say, “Want a different angle?” to keep your brain in gear.</p><p>Training’s a big piece too—workers need to know how to test AI’s work, not just trust it blind. They could practice spotting when AI’s off or mixing its ideas into their own style, like editing a draft to sound less robotic. The study says bosses should teach this so folks get comfy steering AI, not just riding along.</p><p>The point is to design AI that lifts us up, not drags us down. Think of it like a gym buddy—it should push you to flex your brain muscles, not do all the lifting for you. That way, we keep getting smarter, even as AI gets slicker.</p><p>Below is a new Chapter 6 for your "Detailed Research Summary," titled "Two Paths Forward," showcasing two possible approaches to the AI-critical thinking dilemma. It uses the opinions from Kevin Roose and Casey Newton as examples, written in the same simple style with short 2-3 sentence paragraphs, and without formatting.</p><h2>Two Paths Forward</h2><p>The research leaves us with a puzzle: how do we handle AI’s power without losing our own thinking skills? Two tech journalists, Kevin Roose and Casey Newton, have different takes on this. They show us two ways we might tackle the problem.</p><p>Kevin Roose thinks there’s a limit to how much we should let AI take over our brains. He says one day we’ll feel weird about it—like, “Hey, I haven’t had my own idea in weeks!” He imagines people setting aside time to turn off AI and just think solo, keeping their minds fresh.</p><p>For Roose, it’s about drawing a line to protect our own smarts. He worries that if we outsource too much to AI, we might forget how to come up with stuff ourselves. His fix is simple: take breaks from AI to stay original.</p><p>Casey Newton sees it differently—he’s excited about AI being like the world’s best editor on his laptop. He’d ask it, “Help me plan a story, who should I talk to, how do I make it fun?” If AI makes his work awesome, he’s cool with it, even if he didn’t think as hard.</p><p>Newton’s approach is about teaming up with AI to do better stuff. He wonders what value we really bring when AI gets so good—maybe it’s okay to let it do the heavy lifting. To him, the goal is great results, not who did the thinking.</p><p>Roose wants us to guard our own ideas, while Newton wants AI to boost what we can do. Both see the dilemma: AI’s help versus our brains. It’s up to us to pick a path—or mix them—to keep work smart and human.</p></div>
</body>
</html>