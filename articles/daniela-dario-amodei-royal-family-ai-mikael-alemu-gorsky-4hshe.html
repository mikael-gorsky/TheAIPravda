<html>
<head>
  <title>#7: Daniela and Dario Amodei - royal family of AI</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img class="series-logo" src="https://media.licdn.com/dms/image/v2/D4D12AQEfmvWDvno3mg/series-logo_image-shrink_300_300/series-logo_image-shrink_300_300/0/1726763056922?e=1754524800&v=beta&t=yDjwvI9T-dZ4fE5WvZZELb-Qjf3wgc7WRrFECKBQX9Q" />
    <h3>
      <span class="series-title">The AI Pravda</span>
      </br>
      <span class="series-description">Generative AI will change the world as we know it. Let's get prepared. </span>
    </h3>
    <img src="https://media.licdn.com/mediaD4D12AQFStUE90EQWHA" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/daniela-dario-amodei-royal-family-ai-mikael-alemu-gorsky-4hshe">#7: Daniela and Dario Amodei - royal family of AI</a></h1>
    <p class="created">Created on 2024-10-07 15:42</p>
  <p class="published">Published on 2024-10-09 06:00</p>
  <div><p>The Shakespearean story of brother and sister ruling a rebel kingdom comes to life when we learn origins of Anthropic – the most interesting Generative AI company at the moment. Their large language model called Claude-2 is a de-facto standard solution for any serious research task that requires online access and ability to process large texts.</p><p>Anthropic was founded two years ago by Daniela and Dario Amodei who have both involved as core team members of OpenAI. They left OpenAI due to “directional differences”, specifically regarding OpenAI's ventures with Microsoft in 2019. “A rebel AI group”, as they were called by Financial Times, have raised 1.5 billion dollars and present serious threat to the OpenAI’s domination in the hottest technology sphere of our time.&nbsp;</p><p>Dario Amodei does not like too much publicity, but Dwarkesh Patel had made great interview with him on August 8, 2023, and I am happy to share with you two summaries – brief and extended – of that interview. Claude-2 have helped tremendously, of course.</p><pre></pre><ul><li><p>Amodei was one of the first to recognize that simply scaling up compute and data leads to predictable improvements in AI capabilities. However, we still don't fully understand why this smooth scaling occurs.&nbsp;</p></li><li><p>Current AI models are much smaller than the human brain, yet require orders of magnitude more training data to reach similar performance levels. This discrepancy remains a mystery, though multimodal inputs likely play a key role in human learning.</p></li><li><p>Algorithmic innovations like transformers are not viewed as fundamentally enhancing the power of models. Rather, they remove artificial constraints that were holding back capabilities.</p></li></ul><pre></pre><ul><li><p>Amodei believes we could have AI systems that can pass a Turing test within 2-3 years given continued scaling. However, this doesn't mean they will be economically useful or pose an existential risk.</p></li><li><p>He is skeptical that embodied or multimodal AI is required for AGI. The flood of textual data available online allows rapid progress through language model pretraining.</p></li><li><p>Specific capabilities emerge unpredictably as scaling continues. We can't anticipate which skills will suddenly "grok" as model size increases.</p></li></ul><pre></pre><ul><li><p>Amodei sees alignment less as solving a technical problem and more about controlling the development process to steer away from undesirable behaviors.</p></li><li><p>Methods like debate, amplification, and constitutional AI provide training dynamics to shape model values. But we won't know if they truly work until applied to advanced systems.</p></li><li><p>&nbsp;Mechanistic interpretability aims to look inside models and assess if their internal state aligns with intended goals. This provides a type of "extended test set" for alignment.</p></li><li><p>It's unclear if concepts like consciousness will emerge in AI systems. If so, it raises challenging ethical questions around moral obligations to AIs.</p></li></ul><pre></pre><ul><li><p>Amodei believes we are 2-3 years away from AI capabilities that could empower large-scale threats like bioterrorism. This motivates work on preventing misuse.</p></li><li><p>For managing AI risk, he advocates shaping development towards cooperation between companies, government oversight, and decentralized control. No one entity should control superhuman AGI.</p></li><li><p>Extreme cybersecurity measures will be needed to prevent theft of advanced AI systems. Data centers must be designed like "bunkers" to stop misuse by bad actors.</p></li></ul><pre></pre><ul><li><p>Anthropic aims for talent density over talent mass. Concentrating top alignment researchers allows frontier model development paired with safety.</p></li><li><p>The company culture minimizes hype and avoids stratospheric visions. They recognize how hard concrete predictions are and stay empirically grounded.</p></li><li><p>Physicists are attracted to Anthropic because of the "scaling laws" view resembling orderly natural phenomena. This talent has proven adept at AI.</p></li><li><p>&nbsp;Their security relies on compartmentalization so no single person can leak all secrets. However, sustained nation-state attacks would still succeed today.</p></li><li><p>The “Long Term Benefit Trust” will steer Anthropic away from misaligned incentives, though practical integration of AGIs requires political legitimacy.</p></li></ul><p>Overall, Amodei provides an insightful survey of the progress, unknowns, and challenges surrounding cutting-edge AI. He advocates sober recognition of risks coupled with pragmatic safety steps instead of hype or alarmism. The interview highlights how alignment is not a singular technical puzzle, but an ongoing process demanding vigilance at each scaling milestone.&nbsp;</p><blockquote><p>=== extended summary ===</p></blockquote><p>INTRODUCTION</p><p>This was a wide-ranging conversation between Dwarkesh Patel and Dario Amodei, the CEO of Anthropic, an artificial intelligence safety company. They discussed numerous important issues around the current state of AI capabilities, techniques for aligning AI systems to human values, challenges in preventing misuse, and Anthropic's own strategy and culture. What follows is an in-depth look at the key points raised in this thought-provoking discussion.</p><p>THE ORIGINS OF THE SCALING HYPOTHESIS</p><p>One of the fundamental conceptual breakthroughs that set Amodei on the path to founding Anthropic was the realization that simply scaling up compute and data leads to smooth and predictable improvements in AI capabilities. This 'scaling hypothesis' is grounded in his early experience training neural networks at Baidu under Andrew Ng. When tasked with building the best speech recognition system, he found consistent patterns. Adding more layers, training for longer, and utilizing more data systematically reduced error rates.</p><p>&nbsp;This led Amodei to generalize the scaling dynamics observed in speech recognition as a universal phenomenon spanning many AI domains. The smooth scaling curves do resemble orderly natural phenomena from physics. But we still lack a fully satisfying theory for why this works. Some invoke power law correlations or manifold dimensions. But the underlying reasons remain mysterious, even if the empirical results are unambiguous.</p><p>THE SAMPLE EFFICIENCY PARADOX</p><p>One area where our theoretical understanding clearly falls short is explaining the immense hunger for data exhibited by AI systems compared to humans. As Amodei highlights, today's largest models comprise only around 10-100 trillion parameters, far smaller than the human brain with its 100 trillion synapses. Yet they require training datasets with hundreds of billions or even trillions of tokens to reach strong but subhuman performance.</p><p>Humans absorb our rich understanding of the world through hundreds of millions of words at most. What accounts for this paradoxical sample inefficiency? Amodei believes our multimodal sensory experience likely provides far more informative signals compared to the narrow textual focus of today's models. This represents one area where architectural advances could yield substantial gains if they better integrate broader modalities.</p><p>THE LIMITS OF ARCHITECTURAL INNOVATION</p><p>While lamenting the lack of theory for scaling dynamics, Amodei is empirical in assessing the actual impact of algorithmic innovations like LSTMs and transformers. Rather than substantially enhancing model capabilities, he views these breakthroughs as removing artificial constraints. RNNs and LSTMs suffered from the inability to connect far back in time. Transformers lifted this limitation, freeing up compute to operate unencumbered.</p><p>But will we need additional architectural leaps on the scale of transformers to reach advanced AI? Amodei suggests the exponential improvements already underway even without new algorithms make major discontinuities unlikely. These would accelerate progress but not fundamentally alter the trajectory. And while surprises may always arise, he does not anticipate hitting any impassable scaling limits before human-level AI.</p><p>PATHS TOWARDS ARTIFICIAL GENERAL INTELLIGENCE</p><p>Given the rapid advances from scaling, Amodei estimates conversational AI systems could plausibly reach a 'Turing test' threshold of indistinguishability from humans within just 2-3 years. However, passing this narrow linguistic benchmark does not imply truly general intelligence. He believes translating these conversational capabilities into economic or existential impacts will take longer.</p><p>Exactly when any given skill emerges also remains stubbornly difficult to predict. Progress is far smoother on average metrics like cross-entropy loss than mastering specific challenging tasks. Amodei compares it to weather forecasting - predicting climate patterns is easier than anticipating each day's conditions. We must resist overextrapolating from pre-AGI systems to how actual human-level AI will develop.&nbsp;</p><p>WHY LANGUAGE MODELS LEAD THE CHARGE</p><p>Pretraining ever-larger language models on internet text has clearly become the dominant paradigm propelling progress. But why language? Here Amodei emphasizes economic and engineering considerations rather than any philosophical rationale. The torrential flood of digitized language data offers a unique opportunity for cheap self-supervised learning. And fine-tuning provides a pathway to specialized skills like translation or question-answering.</p><p>He believes physical embodiment may eventually prove important for some capabilities. But obtaining such experiential data at scale is far more difficult than leveraging raw text. So pragmatism dictates a continued focus on language model scaling until we reach datasets rivaling the internet's billions of words. Only then might simulation or embodiment become feasible avenues.</p><p>DEFINING AND DEVELOPING AI ALIGNMENT</p><p>With advanced AI potentially imminent, how can we ensure safety and alignment with human values? Amodei argues alignment is less about solving philosophical problems and more akin to controlling a developmental process. We must steer away from undersirable behaviors through training dynamics that shape the emergence of model capabilities.</p><p>Methods like debate, amplification, and Constitutional AI exemplify this approach. They impose constraints and incentives encouraging prosocial preferences. But considerable uncertainties surround their effectiveness. We cannot extrapolate from today's limited systems to how well they will scale up. This makes empirical rigor essential as we test principles on increasingly powerful models.</p><p>Amodei highlights mechanistic interpretability as a tool that moves beyond behavior to peer inside models and directly evaluate internal goal representations. Developing an 'x-ray' to assess if a model's structure aligns with intended outcomes provides powerful complementary insights. However, care must be taken not to allow interpretability objectives to interfere with normal training.</p><p>CAUTIOUS CONFIDENCE ON CONTROL</p><p>Amodei believes achieving human-level AI within years is quite plausible given progress, talent, and resources dedicated to the field. But he remains unsure whether associated risks of misalignment or misuse arise on a similar timescale. Here he advocates a nuanced perspective between alarmism and naivete.</p><p>The mere ability for models to cause harm if they had destructive goals does indeed worry Amodei. At the same time, he thinks current systems are still far from the autonomous agency where value misalignment could fully manifest. Their behavioral inconsistencies suggest alignment may not be doomed by default even as capabilities rise.</p><p>Mechanistic inspection will provide vital clues on alignment prospects moving forward. But exactly when models cross critical thresholds into existential danger remains murky. Amodei believes cautions confidence on control is warranted - we must be prepared for hazardous scenarios while recognizing hazards could emerge slower than capabilities.</p><p>SAFETY, SECURITY, AND POLICY</p><p>Beyond technical work directly improving models, Amodei stresses that holistic integration of AI into society is critical for managing risks. He argues some centralized control will be necessary to handle the immense powers these technologies confer. But this control should incorporate democratic oversight and avoid concentrating power within narrow groups.</p><p>No single entity, whether corporate or governmental, should dictate use of superhuman AI. Legitimacy stems from participatory political processes that mediate between stakeholders. Here again we see Amodei favor a balanced perspective, recognizing the dangers of unfettered development yet resisting utopian governance solutions.</p><p>Bolstering technical capabilities with cybersecurity and physical security measures also remains imperative. As models grow more valuable, data centers must be secured like military installations to prevent leaks or theft. Compartmentalization, proactive governmental collaboration, and fostering security norms within the AI field underlie Anthropic's philosophy here - one cannot be too vigilant given the stakes.</p><p>ANTHROPIC'S ORIGINS AND ADVANTAGES</p><p>Internal development of advanced AI rather than relying solely on external interventions grants Anthropic advantages in alignment research according to Amodei. Critically evaluating solutions requires testing on systems at the frontier pushed by commercial pressures. This dynamism enabled by "being in the arena" differentiates their approach.</p><p>Amodei also emphasizes Anthropic's cultural roots. Its founders shared recognition back in 2017 that alignment solutions were indispensable for AI progress. This precipitated a talent concentration strategy - quality over quantity of researchers. Many employees hail from physics where laws govern exponential phenomena, preparing them for the orderly scaling dynamics seen in AI.</p><p>Minimizing hype and avoiding public personas reinforces Anthropic's empirical scientific ethos. They acknowledge uncertainties and eschew futuristic predictions in favor of incremental rigorous engineering. A long-term benefit trust also inoculates against misaligned incentives, helping fulfill their safety mission.</p><p>THE PATH FORWARD</p><p>In conclusion, Amodei leaves us with a nuanced take on the current landscape. The opportunities from AI progress are immense, but accompanying challenges remain substantial as well. He believes we are on the cusp of machines exceeding human conversation abilities within limited domains.</p><p>Yet exactly when and how broader capabilities will unfurl remains shrouded. And it is unclear whether economic or existential impacts will track close behind raw technical proficiency. There are enough grounds for worry that we must treat alignment and security with utmost seriousness. But not so much cause for alarm that exploration should halt.</p><p>With careful governance, ethical technology development, and sustained scientific inquiry into AI foundations, Amodei believes we can navigate the hazards ahead. Anthropic represents one pillar within a multifaceted societal response. A balanced perspective their work embodies - enthusiastic yet wary, ambitious yet grounded - serves as a constructive role model. If conversations like this become more commonplace, we can confront the AI revolution with wisdom, care, and due diligence.</p></div>
</body>
</html>