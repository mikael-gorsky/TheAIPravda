<html>
<head>
  <title>Legal regulation of AI in Europe</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaD4D12AQHbGZpmufqYVw" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/legal-regulation-ai-europe-mikael-alemu-gorsky-1f">Legal regulation of AI in Europe</a></h1>
    <p class="created">Created on 2023-07-23 17:59</p>
  <p class="published">Published on 2023-07-24 06:00</p>
  <div><p>As of mid-2023 there is a multitude of actual/proposed EU regulations on AI. The reason is simple: EU aims to establish itself as a global leader in human-centric and trustworthy AI, and it is doing this through a combination of mandatory requirements for “high-risk AI applications”, voluntary codes of conduct, safety and transparency obligations, and liability rules founded on ethical principles. Here is a brief overview:</p><p><br></p><p><strong>Artificial Intelligence Act (April 2021)</strong> —&nbsp;proposed by European Commission as part of its ambition to make the EU a global leader in ethical AI.&nbsp;</p><p>The Act introduces harmonized rules for the development, placement on the market and use of AI systems in the EU, it pplies to providers of AI systems in the EU, users of AI systems in the EU, and providers and users of AI systems that are located in a third country, if the output produced by the system is used in the EU. The Act prohibits certain artificial intelligence practices that pose unacceptable risk:</p><p>1. AI systems considered a clear threat to the safety, livelihoods and rights of people will be banned. This includes AI systems or applications that manipulate human behavior to circumvent users' free will (with exceptions for prevention of self-harm), exploitation of children and vulnerable people, social scoring by governments, and use of 'real-time' remote biometric identification in public spaces (with exceptions for security and medical emergencies).</p><p>2. High-risk AI applications will be subject to strict obligations before they can be put on the market:</p><ul><li>High-risk AI systems defined based on the intended purpose of the AI system, sectorial requirements, the degree of harm that could result from the system, and the ability of the system to interact with humans or otherwise manipulate the environment. They include AI applications in critical sectors like health, transport, energy, and parts of the public sector.</li><li>Such systems must meet requirements related to data and data governance, documentation and record keeping, transparency and provision of information to users, human oversight, robustness, accuracy and cybersecurity.</li><li>Users must monitor systems on an ongoing basis through governance mechanisms like quality management systems.</li></ul><p>3. Limited risk AI applications will need to comply with transparency obligations without undergoing ex-ante conformity assessments.</p><ul><li>Providers must ensure systems are transparent, provide clear information to users, and enable human oversight.</li><li>Includes wellness, accessibility tools, conversational systems.</li><li>Most AI will fall under this category.</li></ul><p>4. Voluntary codes of conduct to strengthen transparency and compliance with ethical principles are encouraged for non-high risk AI systems.</p><ul><li>The EU and national authorities will support development of codes by industries and other stakeholders through facilitating sharing of expertise and networking.</li><li>Codes of conduct can specify sector-specific or use-case-specific requirements.</li></ul><p>Timeline for the Artificial Intelligence Act:&nbsp;</p><p>a) in April 2021 European Commission proposed the Artificial Intelligence Act;</p><p>b) 2022-2023 is the targeted timeline for legislative process and negotiations on the Act;</p><p>c) the Act will enter into force 20 months after final publication in Official Journal of the EU. Provisions will be applicable after broad transition periods.</p><p><br></p><p><strong>Artificial Intelligence Liability Directive (August 2021)</strong> - proposed by European Parliament's Special Committee on Artificial Intelligence in a Digital Age and introduces civil liability regime for damage caused by AI systems when operating autonomously and interacting with third parties. Key aspects:</p><p>1. Strict liability for operators of high-risk AI systems that cause damage to life, health or physical integrity of person or damage to property. Operators only exonerated in cases of force majeure, third party interference, or the person harmed intentionally acted to cause damage.</p><p>2. Fault-based liability for other AI systems - liability if operator fails to comply with duties of care.</p><p>3. Special liability regime for AI systems exceeding certain capabilities like self-learning. Operators strictly liable for damage unless they can prove that the AI systems' design enabled compliance with duties of care.</p><p>4. Personality rights proposed for AI systems exceeding certain capabilities that indicate human qualities like consciousness. Enables damages for harm to these rights. &nbsp;</p><p>5. National supervisory authorities proposed to monitor application of regulation.</p><p>Timeline for Artificial Intelligence Liability Directive:</p><p>a) August 2021 - Proposed by European Parliament Committee;</p><p>b) Legislative process in European Parliament and Council ongoing.</p><p><br></p><p><strong>Ethics Guidelines for Trustworthy AI (April 2019) </strong>- developed by High-Level Expert Group on Artificial Intelligence set up by the European Commission in 2018 and identifies 7 key requirements that AI systems should meet to be considered trustworthy:</p><p>1. Human agency and oversight: ability to oversee AI system through governance mechanisms.</p><p>2. Technical robustness and safety: resilience to risks, ability to reliably work.</p><p>3. Privacy and data governance: ensuring proper privacy protection and data management.</p><p>4. Transparency: traceability, explainability and communication of AI system.&nbsp;</p><p>5. Diversity, non-discrimination and fairness: avoidance of unfair bias, accessibility.</p><p>6. Societal and environmental well-being: ensuring sustainability.</p><p>7. Accountability: mechanisms to ensure responsibility and accountability for AI systems and their outcomes.</p><ul><li>Provides guidance for implementing these principles.</li><li>Preceded development of concrete legislative initiatives on AI by outlining ethical underpinnings.</li></ul><p>Timeline for Ethics Guidelines for Trustworthy AI:&nbsp;</p><p>a) April 2019 - Guidelines published based on work of expert group from 2018 onwards;</p><p>b) Provides foundation for upcoming legislative actions.</p><p><br></p><p>It would be safe to assume that one of the most significant discussions that regulator have are the discussions around the transparency requirements for AI systems, especially complex machine learning models. The black box nature of many AI systems like deep neural networks poses challenges for transparency. However, the regulations emphasize the importance of traceability and explainability of AI systems to ensure trust.</p><p>Some key aspects of the regulatory issues in regard with the transparency of AI systems:</p><ul><li>The Artificial Intelligence Act requires high-risk AI systems to be designed and developed in a manner that allows for humans to interpret the system's functioning and outputs. Technical documentation must be maintained to demonstrate compliance.</li><li>For limited-risk AI systems, the provider must ensure the system is transparent, provides clear information, and enables human oversight. But no conformity assessment is required.</li><li>The Ethics Guidelines also highlight the need for traceability, explainability and communication related to AI systems. But acknowledge there are limits to transparency depending on the complexity of the system.</li><li>The Guidelines recommend that at a minimum, AI systems should be interpretable to developers and meaningful information provided to users. But there are disagreements over the level of explainability required, especially for public sector systems.</li><li>The difficulty of applying transparency requirements to complex, self-learning algorithms like neural networks is recognized. Alternate methods like model cards, code transparency and algorithm auditing are emerging solutions.</li><li>There are calls for proportionate obligations based on risk, use case and context. Right to explanation as part of fundamental rights is debated.</li></ul><p><br></p><p>We see that a transparency of AI systems is a complex topic under ongoing discussion. The regulations aim to strike a balance between ensuring interpretability, traceability and oversight while recognizing the technology's limitations. But a very significant question remains open: HOW developers of AI can satisfy regulators, when every complex machine learning model tend to increase its complexity by itself without developers’ input. Here are some ways companies could potentially provide traceability for their AI systems as per discussions around the proposed regulations:</p><ol><li>Maintain extensive documentation and technical descriptions of the AI system's development process, including its design choices, development methodology, data sources and labels, testing procedures, validation results, and governance.</li><li>Implement algorithm auditing mechanisms and maintain version histories so each update to the system is documented.</li><li>Adopt explainable AI (XAI) techniques like generating explanations along with outputs, developing interpretable models, or using example-based explanations. But these have limitations for complex AI like deep learning.</li><li>For machine learning models, maintain metadata around features, hyperparameters, model architecture, and training data. Explain broader model insights even if individual predictions are hard to interpret.</li><li>Provide model cards that outline intended use cases, key metrics, safety considerations, social context, and other details about the AI system.</li><li>Implement measures for recording decisions/recommendations made by the AI system along with the inputs, outputs and relevant context for each decision.</li><li>Testing AI systems through techniques like systematically evaluating different model versions, input perturbations, and simulating user scenarios to assess behaviors.</li><li>External algorithm auditing by independent third parties to evaluate properties and reliability of the AI system.</li><li>Adopt standards like DARPA's Explainable AI framework which focuses on traceability across the AI lifecycle.</li></ol><p><br></p><p>In practice, a combination of technical documentation, logging, explainability methods, testing, and auditing mechanisms could provide degrees of traceability for an AI system based on its nature and risk level. But significant open challenges remain for more complex systems.</p></div>
</body>
</html>