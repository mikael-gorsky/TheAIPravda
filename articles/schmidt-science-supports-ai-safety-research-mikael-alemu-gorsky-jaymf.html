<html>
<head>
  <title>Schmidt Science supports AI safety research</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img class="series-logo" src="https://media.licdn.com/dms/image/v2/D4D12AQEfmvWDvno3mg/series-logo_image-shrink_300_300/series-logo_image-shrink_300_300/0/1726763056922?e=1754524800&v=beta&t=yDjwvI9T-dZ4fE5WvZZELb-Qjf3wgc7WRrFECKBQX9Q" />
    <h3>
      <span class="series-title">The AI Pravda</span>
      </br>
      <span class="series-description">Generative AI will change the world as we know it. Let's get prepared. </span>
    </h3>
    <img src="https://media.licdn.com/mediaD4D12AQGbnbNdNTUxnQ" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/schmidt-science-supports-ai-safety-research-mikael-alemu-gorsky-jaymf">Schmidt Science supports AI safety research</a></h1>
    <p class="created">Created on 2025-02-22 16:21</p>
  <p class="published">Published on 2025-02-24 05:30</p>
  <div><p>When governments decide to sleep at the wheel, private philanthropy steps in.&nbsp;</p><p>We see this in global healthcare, where the Gates Foundation boosted support to the Global Alliance for Vaccines and Immunization (GAVI) by tens of millions of dollars following the latest shifts in US policy. And in AI safety research, <a target="_blank" href="https://www.linkedin.com/company/schmidt-sciences/">Schmidt Sciences</a>,&nbsp;a nonprofit organization founded by Eric and Wendy Schmidt, is launching the AI Safety Science program with a budget of $10 million.</p><p>Eric Schmidt's involvement in AI safety isn't new. As the former CEO of Google, he has long been engaged with the promises and perils of artificial intelligence. His recent book with Henry Kissinger and Craig Mundie i”s called “Genesis: Artificial Intelligence, Hope, and the Human Spirit”, and it demonstrates his nuanced understanding of AI's potential to solve humanity's greatest challenges and pose unprecedented risks.</p><p>The timing of Schmidt Sciences' initiative is critical. US government funding for AI safety research will likely disappear after VPOTUS’s words at the AI Action Summit. Meanwhile, AI systems are rapidly being integrated into every sector of society, from healthcare to government operations.</p><p>The team at Schmidt Sciences has highlighted three key Issues in AI Safety Research:</p><blockquote><p>1. Inadequate Safety Benchmark Ecosystem:</p></blockquote><p>- Too few well-founded, quality benchmarks</p><p> - High correlation between existing benchmarks, suggesting they don't assess independent capabilities</p><p>- Lack of evaluations for agentic and multi-modal capabilities</p><p>- Need for proactive safety measures for future, more sophisticated systems</p><blockquote><p>2. Insufficient Research Funding:</p></blockquote><p>- Total funding is estimated at only $80-130 million per year (2021-2024), a tiny fraction of what's invested in AI development overall</p><p>- Government programs like Safe Learning-Enabled Systems offering only $10 million annually</p><p>- Current funding levels prohibit larger, long-term research efforts</p><blockquote><p>3. Underutilization of Academic Expertise:</p></blockquote><p>- Safety research on the largest models mainly conducted by frontier AI labs</p><p>- Academic community lacks access to frontier models, testing environments, and compute resources</p><p>- Need for more peer review and reproducible research in AI safety</p><p>Schmidt Sciences is addressing this challenge by funding multiple pioneering research projects. Please see descriptions of several projects:</p><p><a target="_blank" href="https://www.linkedin.com/in/yoshuabengio?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAUyvxABF23qZBS5f4xTAqqAqO3Yd4MHbqQ">Yoshua Bengio</a> <strong>(Mila - Quebec AI Institute)</strong></p><p><em>- Developing quantitative safety guardrails for frontier AI models</em></p><p><strong>Adam Gleave, Kellin Pelrine (</strong><a href="http://FAR.AI" target="_blank"><strong>FAR.AI</strong></a><strong>) &amp; Thomas Costello (American University and MIT)</strong></p><p><em>- Assessing AI-driven deception and disinformation risks and developing mitigations</em></p><p><strong>Tatsu Hashimoto (Stanford University)</strong></p><p><em>- Creating testing environment with statistical guarantees for AI agent failure rates</em></p><p><em>- Building safety metrics to identify emergent unsafe capabilities</em></p><p><a target="_blank" href="https://www.linkedin.com/in/ACoAAAraWJABPQZ2ligQH_IP9BXiKHhuVfNmyyE?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAraWJABPQZ2ligQH_IP9BXiKHhuVfNmyyE">Matthias Hein</a> <strong>(University of Tübingen) &amp; </strong><a target="_blank" href="https://www.linkedin.com/in/ACoAADITpGAByt5lHTtryS5cOQB6dc7eXJb8c5E?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADITpGAByt5lHTtryS5cOQB6dc7eXJb8c5E">Jonas Geiping</a> <strong>(ELLIS Institute Tübingen)</strong></p><p><em>- Investigating degradation of AI system safety over extended operation periods</em></p><p><a target="_blank" href="https://www.linkedin.com/in/ACoAAAkio-oBPSKGbOFxWEvr6Nq5ALFlRL5PyJc?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAkio-oBPSKGbOFxWEvr6Nq5ALFlRL5PyJc">Daniel Kang</a> <strong>(University of Illinois, Urbana-Champaign)</strong></p><p><em>- Assessing AI agents' capability to perform complex cybersecurity attacks</em></p><p><a target="_blank" href="https://www.linkedin.com/in/ACoAABYfvJgBJleUy0X6j9_x1Ali-KX0HC9OGyM?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAABYfvJgBJleUy0X6j9_x1Ali-KX0HC9OGyM">Zico Kolter</a> <strong>(Carnegie Mellon University) </strong></p><p><em>- Exploring causes of adversarial transfer between AI models</em></p><p><a target="_blank" href="https://www.linkedin.com/in/lxbosky?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAZvofsBIX9xG_1VjWGMQP3MP7dpA_1L3Os">Bo Li</a> <strong>(University of Illinois Urbana-Champaign)</strong></p><p><em>- Designing virtual environment with advanced red teaming algorithms for AI system evaluation </em></p><p><a target="_blank" href="https://www.linkedin.com/in/evan-miyazono?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAKVD4MBeXXBz_paWnpxXlSRHK22T-yprlg">Evan Miyazono</a><strong> &amp; </strong><a target="_blank" href="https://www.linkedin.com/in/ACoAAAgGOMEBdDwFqZP_tdDP28TjogeAzpdmN_o?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAgGOMEBdDwFqZP_tdDP28TjogeAzpdmN_o">Daniel Windham</a> <strong>(Atlas Computing) </strong></p><p><em>- Developing AI tool for generating, reviewing, and validating formal code specifications </em></p><p><a target="_blank" href="https://www.linkedin.com/in/karthiknarasimhan1?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAA3oPCMBScENDpJ7Zld6Jb5p8th7aHbncwg">Karthik Narasimhan</a> <strong>(Princeton University)</strong></p><p><em>- Developing benchmarks to evaluate reliability and consistency of AI code-writing systems</em></p><p><a target="_blank" href="https://www.linkedin.com/in/randomwalker?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAD85EvsBeNijbxJLxmNZcA4cF5Gc0JEwrc4">Arvind Narayanan</a> <strong>(Princeton University)</strong></p><p><em>- Designing verifiers to check AI system accuracy and improve agent reliability</em></p><p><a target="_blank" href="https://www.linkedin.com/in/floriantramer?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAABMlpKkBSTIdht5rVsozp4ag54m9jq2c1os">Florian Tramèr</a> <strong>(ETH Zurich)</strong></p><p><em> - Designing guardrails against prompt injection attacks</em></p><p><em>- Developing formal programming framework to standardize these guardrails</em></p><p>These projects will receive not just financial support, but also crucial resources like computational power and API access. This support is particularly important for academic researchers, who have been largely left out of AI safety research despite their potential to contribute valuable insights.</p><p>Through this program, Schmidt Sciences is planting the seeds for safer AI development in academic soil. Their goal aligns with the vision presented in "Genesis" - to chart a course between blind faith and unjustified fear in the age of AI, while ensuring this powerful technology benefits humanity while protecting us from potential harms.</p></div>
</body>
</html>