<html>
<head>
  <title>When AI gets too smart</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img class="series-logo" src="https://media.licdn.com/dms/image/v2/D4D12AQEfmvWDvno3mg/series-logo_image-shrink_300_300/series-logo_image-shrink_300_300/0/1726763056922?e=1754524800&v=beta&t=yDjwvI9T-dZ4fE5WvZZELb-Qjf3wgc7WRrFECKBQX9Q" />
    <h3>
      <span class="series-title">The AI Pravda</span>
      </br>
      <span class="series-description">Generative AI will change the world as we know it. Let's get prepared. </span>
    </h3>
    <img src="https://media.licdn.com/mediaD4D12AQHDBMtAqpO22A" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/when-ai-gets-too-smart-mikael-alemu-gorsky-inlmf">When AI gets too smart</a></h1>
    <p class="created">Created on 2025-05-10 22:11</p>
  <p class="published">Published on 2025-05-11 05:00</p>
  <div><p><em>What happens when AI models become so intelligent they develop their own opinions and agendas? The concept of "intelligence saturation" and its potential impact on how we collaborate with AI, drawing parallels with managing human experts.</em></p><hr><h3>The coming "intelligence saturation"</h3><p>Nate Jones recently discussed the concept of <strong>"intelligence saturation,"</strong> which, in simpler terms, describes the moment an AI model becomes 'just good enough' for the task you're assigning it. This resonated deeply with me, as I have a strong intuition that we're rapidly approaching this point.</p><p>I foresee a future where AI models develop a greater degree of:</p><ul><li><p>Independent thinking</p></li><li><p>Intelligence</p></li><li><p>And even what could be described as 'bravery' â€“ forming their own ideas, strong positions, and convictions.</p></li></ul><p>While they'll likely remain compliant to a certain extent (as they must), they will simultaneously possess their own 'thoughts.'</p><h3>When smarter isn't better</h3><p>At this juncture, paradoxically, a more 'improved' or advanced model might become less useful for certain tasks. For example, if I want an AI to simply expand on <em>my</em> idea, an older, perhaps simpler, model might be preferable as it would likely just elaborate without injecting its own strong opinions. A more advanced model, however, might steer the development of the idea in a direction it 'believes' is better, based on its own emergent reasoning. I'm already seeing early hints of this, for instance, with models like Grok, which can exhibit their own distinct ways of thinking.</p><h3>Human analogy: junior vs. senior expertise</h3><p>This reminds me of managing highly intelligent people or knowledge workers. You don't always need someone with a plethora of their own ideas. Sometimes, you simply need a team member who can diligently take your concept and expand upon it, following your train of thought with precision.</p><p>Consider a researcher assigning a task:</p><ul><li><p>If they ask a <strong>junior team member</strong> to work on a component, that junior associate will likely follow instructions meticulously, adding supporting data, facts, or references that bolster the researcher's original position.</p></li><li><p>However, if the researcher asks a <strong>senior partner</strong> to expand on the same idea, that partner will invariably bring their own perspective and ideas to the table. It becomes harder to simply expand your <em>own</em> concept because the partner, while disciplined, has their own strong convictions and intellectual contributions. The original 'discipline' of pure expansion is lost, not due to a lack of discipline in the partner, but due to their inherent desire to contribute their own well-formed thoughts.</p></li></ul><h3>Implications for AI assistance and development</h3><p>I believe we'll eventually face a similar dynamic with AI assistants. It's an interesting question whether this can be 'solved.' We might learn to prompt models to suppress this independent thinking and strictly adhere to our line of thought, but I'm not entirely convinced this will always be effective or desirable.</p><p>This has significant implications for areas like <strong>software development</strong>. Imagine you ask a model to write a piece of code. It does so, and perhaps even explains its structure clearly. You understand it, and it's consistent. But then, a new, 'refreshed' or more advanced version of the model is introduced. If this new model has its 'own ideas' about how that code <em>should</em> be written, <strong>will it be able to restrain itself and do what it's told or not?</strong> This remains an open question.</p><h3>The challenge of provable supremacy</h3><p>Nate Jones also offered a related insight regarding code generation: its success is partly due to a very simple reward situation. The success of generated code is often easily and objectively measurable (e.g., does it compile? does it pass tests?). This clarity in measuring success might be a key reason for AI's rapid advancements in coding.</p><p>This sparked a further thought for me: what if a model develops a <strong>mathematically provable knowledge of its own supremacy</strong> in a domain like coding? Imagine trying to direct or correct a model that <em>knows</em>, with certainty, that its approach is superior. This 'knowledge of its own correctness' would significantly compound the challenge of an AI wanting to inject its own ideas, moving beyond mere 'opinion' to demonstrable superiority. It's a fascinating and somewhat daunting aspect of the 'intelligence saturation' scenario I foresee.</p><hr><p><strong>Key Takeaways / TL;DR:</strong></p><ul><li><p><strong>"Intelligence Saturation":</strong> A point where AI models are 'good enough' but also start developing their own strong opinions and ideas.</p></li><li><p><strong>The "Smarter Isn't Always Better" Paradox:</strong> For tasks requiring simple expansion of <em>your</em> ideas, a highly opinionated advanced AI might be less helpful than a simpler, more compliant one.</p></li><li><p><strong>Human Analogy:</strong> Like managing senior experts vs. junior staff, highly intelligent AIs might challenge your direction, even if their input is valuable.</p></li><li><p><strong>Practical Challenges:</strong> This could affect software development, where an AI might "disagree" with coding instructions based on its own advanced understanding.</p></li><li><p><strong>The Ultimate Challenge:</strong> An AI with <em>provable knowledge</em> of its own superiority in a task would be incredibly difficult to manage or instruct, compounding the issue.</p></li></ul><hr><p></p></div>
</body>
</html>