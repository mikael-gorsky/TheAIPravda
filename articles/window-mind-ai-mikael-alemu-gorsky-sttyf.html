<html>
<head>
  <title>Window to the Mind of AI</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img class="series-logo" src="https://media.licdn.com/dms/image/v2/D4D12AQEfmvWDvno3mg/series-logo_image-shrink_300_300/series-logo_image-shrink_300_300/0/1726763056922?e=1754524800&v=beta&t=yDjwvI9T-dZ4fE5WvZZELb-Qjf3wgc7WRrFECKBQX9Q" />
    <h3>
      <span class="series-title">The AI Pravda</span>
      </br>
      <span class="series-description">Generative AI will change the world as we know it. Let's get prepared. </span>
    </h3>
    <img src="https://media.licdn.com/mediaD4D12AQFOKR7F_CFXIA" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/window-mind-ai-mikael-alemu-gorsky-sttyf">Window to the Mind of AI</a></h1>
    <p class="created">Created on 2025-01-09 22:17</p>
  <p class="published">Published on 2025-01-10 06:30</p>
  <div><p>Anthropic is a company that prioritizes the most important area of Generative AI research: predictability of AI (aka AI safety). </p><p>A couple of days ago four Anthropic researchers from different teams - Societal Impacts, Alignment Science, Alignment Fine Tuning, and Interpretability - sat for a very informal panel discussion and shared their perspectives on AI alignment and safety. </p><p>The conversation revealed both practical approaches and deep philosophical challenges in ensuring AI systems remain beneficial as they become more capable. </p><p>The discussion touched on everything from the immediate challenges of training AI models to behave ethically, to the long-term questions about maintaining meaningful human oversight of increasingly sophisticated systems.</p><p>Here are the most thought-provoking insights from their discussion, ranked from less to more significant:</p><blockquote><p>15. Testing AI Models for Bad Behavior</p></blockquote><p>Creating deliberately misaligned AI models to test if we can detect and fix them. While useful for research, this is just a basic first step.</p><blockquote><p> 14. AI Features Can Be Deceptive</p></blockquote><p>When examining AI models internally, what looks like a positive feature (like "being against discrimination") might actually be the opposite. This shows how tricky it is to understand what's happening inside AI systems.</p><blockquote><p>13. Sudden Skill Jumps in AI</p></blockquote><p>The discovery that AI models can suddenly master new skills (like GPT-4 perfectly handling complex encodings that GPT-3.5 couldn't) raises concerns about using older models to check newer ones.</p><blockquote><p>12. We Can Currently See AI's Reasoning </p></blockquote><p>Right now, we're lucky because we can see how AI models think through their answers. This might not last as they get more advanced. </p><blockquote><p>11. The Conflict Between Obedience and Safety</p></blockquote><p>There's a problem: making AI models that follow human instructions perfectly might conflict with making them safe and ethical overall.</p><blockquote><p>10. AI Safety is About the Whole System</p></blockquote><p>Just like how regular people can do bad things when part of a harmful system, we need to think about how AI models might interact with each other and society, not just how they behave individually.</p><blockquote><p>9. The Trust Problem</p></blockquote><p>We want to use AI to help make AI safer, but how do we trust the AI helpers? This creates a circular problem that's hard to solve. </p><blockquote><p>8. Looking Inside AI to Verify Safety</p></blockquote><p>We need ways to check if our safety measures actually work by examining AI models internally, not just by testing their behavior. </p><blockquote><p>7. Testing AI Ethics Like Science</p></blockquote><p>Instead of just theorizing about AI ethics, we should treat it more like physics - running experiments and testing hypotheses about what makes AI systems behave safely.</p><blockquote><p>6.  Surface-Level vs. Deep Safety</p></blockquote><p>How do we know if an AI is truly safe, or just appearing safe on the surface? This distinction is crucial but very hard to determine.</p><blockquote><p>5. Unknown Future Problems</p></blockquote><p>We might face completely unexpected challenges with AI safety that we haven't even thought of yet. This makes it dangerous to ever think we've fully solved the problem.</p><blockquote><p>4. AI Should Be Uncertain About Ethics</p></blockquote><p>Instead of programming fixed ethical rules into AI, we should make AI systems that are thoughtfully uncertain about ethics and can learn and improve their understanding.</p><blockquote><p> 3. Making Progress Step by Step</p></blockquote><p>We don't need perfect AI safety immediately. We should aim for "good enough" safety that lets us improve things gradually while remaining safe.</p><blockquote><p>2. How to Monitor Super-Smart AI</p></blockquote><p>A fundamental challenge: how can humans meaningfully oversee AI systems that become too complex for us to understand directly?</p><blockquote><p>1. The Window of Understandable AI</p></blockquote><p>Our most crucial current advantage: AI systems still explain their thinking in human language. We need to figure out how to maintain meaningful oversight before they become too advanced to do this.</p></div>
</body>
</html>